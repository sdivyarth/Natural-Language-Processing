{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import tensorflow as rf\n",
    "from tensorflow.keras.layers import Dense, Input,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer,TFBertModel\n",
    "from transformers import DistilBertTokenizer,TFDistilBertModel,DistilBertConfig,TFBertForPreTraining\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes HTML tags;\n",
    "#converts to lower case; \n",
    "#converts 'positive' -> 1 , 'negative' -> 0\n",
    "\n",
    "\n",
    "class PreProcess :\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df=df\n",
    "        \n",
    "    \n",
    "    def pre_process(self):\n",
    "        self.df['review']=self.df['review'].str.replace(r'<[^<]+?>', '')\n",
    "        self.df['review']=self.df['review'].str.replace(r\"\\\\\", '')\n",
    "        self.df['review']=self.df['review'].str.lower()\n",
    "        self.df['sentiment']=self.df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "        return self.df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds tokens column to the dataframe\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self,df,model='distilbert-base-uncased',max_length=128):\n",
    "        \n",
    "        self.df=df\n",
    "        self.max_length=max_length\n",
    "        self.model=model\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(self.model)\n",
    "    \n",
    "    def get_dict(self):\n",
    "        \n",
    "        def helper(sentence):\n",
    "            \n",
    "            ############# add_special tokens add [CLS], [SEP] etc.\n",
    "            ############# max_lenght : determine the maximum length of the sentence\n",
    "            ############# pad_to_max_length : add 0's to sentences less than pad length\n",
    "            ############# return atttention mas: a dictionary telling if the token_id is a padded one or real one\n",
    "\n",
    "            return dict(self.tokenizer.encode_plus(sentence,add_special_tokens = True, max_length = self.max_length, pad_to_max_length = True,  return_attention_mask = True))\n",
    "        \n",
    "        return self.df['review'].apply(lambda x: helper(str(x)))\n",
    "        \n",
    "    def tokenize(self):\n",
    "        \n",
    "        token_dict=self.get_dict()\n",
    "        dct={'input_ids':[],'attention_mask':[]}\n",
    "        \n",
    "        for tok in token_dict:\n",
    "            \n",
    "            dct['input_ids'].append(tok['input_ids'])\n",
    "            dct['attention_mask'].append(tok['attention_mask'])\n",
    "            lst=np.concatenate(([np.array(dct['input_ids']),np.array(dct['attention_mask'])]),axis=1)\n",
    "            lst=lst.reshape(len(lst),2,self.max_length)\n",
    "            \n",
    "        return tf.convert_to_tensor(dct['input_ids']),tf.convert_to_tensor((dct['attention_mask']))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    trans=TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    input_ids=Input(shape=(max_length,),dtype='int32',name='input_ids')\n",
    "    input_attention_masks=Input(shape=(max_length,),dtype='int32',name='attention_mask')\n",
    "\n",
    "    bert_layer=trans([input_ids,input_attention_masks])[0]\n",
    "\n",
    "    flat_layer = Flatten()(bert_layer)\n",
    "\n",
    "    output_layer = Dense(1, activation='sigmoid')(flat_layer)\n",
    "\n",
    "    model = Model(inputs=[input_ids,input_attention_masks], outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('IMDB Dataset.csv',nrows=10)\n",
    "\n",
    "#Preprocess Dataframe\n",
    "pp=PreProcess(df)\n",
    "df=pp.pre_process()\n",
    "\n",
    "#train test split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#Tokenize\n",
    "y_train,y_test=df_train.sentiment,df_test.sentiment\n",
    "token_train=Tokenizer(df_train)\n",
    "token_train=token_train.tokenize()\n",
    "token_test=Tokenizer(df_test)\n",
    "token_test=token_test.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model()\n",
    "model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(token_train,y_train,validation_split=0.05,epochs=1,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(token_test)\n",
    "y_pred=np.array(list(map(lambda x: 1 if x>=0.5 else 0,y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
